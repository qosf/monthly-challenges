{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 05: Quantum Machine Learning and Quantum Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_This challenge is brought to you by [@Shiro-Raven](https://github.com/Shiro-Raven) as a community contribution. Thank you!_**\n",
    "\n",
    "In [Challenge 03](../challenge-03/qosf-monthly-challenge-03.ipynb), QAOA was one of the proposed algorithms to solve the problem statement. This algorithm, along with many others known as **variational algorithms**, makes use of a classical controller to optimise the parameters of the different parametric operators that can be found in the circuit. Variational algorithms have many uses in the area of quantum computing, one of which we will take a look at today: Quantum Machine Learning (QML).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A Quick ML Intro\n",
    "For those unfamiliar with classical ML, don't fret: it's not that complicated. Machine Learning is a huge family of algorithms and techniques that attempt to find hidden relationships inherent in and statistical distributions from the data. For example, assume you have a table of school grades, and you want to predict whether a given student will end up joining a scientific major or more of a humanitarian one. Typically, you would check his grades in physics, chemistry and math, and compare them to his grades in literature and art. A trivial prediction would be to favour the subjects that relate to the respective major the most: so a student acing all the math, chemistry, and physics grades is predicted to end up choosing a scientific major. \n",
    "\n",
    "A lot of libraries nowadays offer a \"black-box\" solution for all your ML needs. You simply pick one of the algorithms the library provides, throw in the data, and out comes your model. No sweat!\n",
    "\n",
    "For a more lengthy and comprehensive intro, you can check resource [A]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why QML though?\n",
    "\n",
    "Even though classical ML and the so-called deep neural networks have contributed significantly to many disciplines they are not perfect. Even state-of-the-art algorithms may require a significant amount of training time given a huge volume of data present. While QML also would be affected by a huge volume of data, one potential application of QML is to improve training by leveraging quantum phenomena. If the data happens to be quantum in nature, then QML can shine if run on quantum computers where it might be expensive to simulate on classical computers.\n",
    "\n",
    "Another limitation of classical ML (notably classical neural networks) is their degrees of freedom. With the classical bits being restricted to only two states, the only way to add a new degree of freedom is to add more classical bits. However, this approach has limited expressibility. Qubits, on the other hand, have a secret magical weapon: Superposition! This weapon adds a very powerful degree of freedom which can encode data more efficiently and thus allow for a better expressibility with fewer resources. You can read through reference [2] for a technical discussion of this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this challenge to experiment around with simple QML models, using them for several simplistic classification tasks on classical data. Typically, a QML circuit would consist of three main stages, shown in the image below resource [B]:\n",
    "\n",
    "![Stages of QML](./qml.png)\n",
    "\n",
    "The first step is about encoding/embedding the classical data into qubits, after that, the parameterized circuit does its magic, manipulating the state of the qubit before carrying out measurement, and using the result to optimise the parameters using a classical optimiser. For some cases, as shown in resource [B], the embedding stage can be parameterized as well, making the second stage obsolete. Consequently, it would be sufficient and cheaper to only deal with two stages. For different approaches and technical discussion of embedding data, check resource [B] and reference [3].\n",
    "\n",
    "Without further ado, the tasks of this challenge are as follows:\n",
    "\n",
    "1. Starting out with something very simple, follow this [tutorial](https://pennylane.ai/qml/demos/tutorial_variational_classifier.html) from PennyLane to carry out classification tasks on the benchmark [Iris dataset](https://archive.ics.uci.edu/ml/datasets/iris). The tutorial will start out with a simpler example and then build up to classifying the Iris dataset. Feel free to experiment with different circuits (either for embedding or manipulation) other than those presented in the tutorials and see how they fare with the same test dataset (resource [B] is a good place to start). ***Bonus***: Present your findings using nicely visualised graphs, demonstrating the learning curves and final accuracies.\n",
    "\n",
    "2. Now that you got a feel of implementing a QML model, let's go for something more challenging. The [MNIST dataset](https://www.kaggle.com/c/digit-recognizer/data) is another benchmark dataset that is typically used to check the sanity of Computer Vision models. Dealing with this dataset is tricky for two reasons: it is highly-dimensional (28 x 28 pixels = 784 dimensions!), and it contains way more classes than the Iris dataset. Think about what you can do to use as few qubits as possible (Hint: think of it as using as few \"dimensions\" as possible. If you give up, and can't think of a way, check resource [C]). As for the higher number of classes, it is okay to start with only two arbitrary classes, excluding all the others, and adding more classes as you feel comfortable with your approach.  \n",
    "\n",
    "3. ***Bonus***: Experiment with quantum autoencoders (reference [4]), which mirror classical autoencoders, to compress the embedded information from part 2 even further. See how few qubits you can get away with while still retaining good accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[A] [Machine Learning Tutorial for Beginners: What is, Basics of ML](https://www.guru99.com/machine-learning-tutorial.html)\n",
    "\n",
    "[B] [A nice lecture series about QML](http://nithep.ac.za/training/)\n",
    "\n",
    "[C] [Hint for part 2](https://www.datacamp.com/community/tutorials/principal-component-analysis-in-python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] [An introduction to quantum machine learning](https://arxiv.org/abs/1409.3097)\n",
    "\n",
    "[2] [The power of quantum neural networks](https://arxiv.org/abs/2011.00027)\n",
    "\n",
    "[3] [Quantum embeddings for machine learning](https://arxiv.org/abs/2001.03622)\n",
    "\n",
    "[4] [Quantum autoencoders for efficient compression of quantum data](https://arxiv.org/abs/1612.02806)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
